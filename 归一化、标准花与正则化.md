## 0x01 归一化 Normalization
归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。

常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 Min-Max 归一化：

## Min-Max 归一化
$$x_{new} = \frac{x-x_{min}}{x_{max}-x_{min}}$$
举个例子，我们判断一个人的身体状况是否健康，那么我们会采集人体的很多指标，比如说：身高、体重、红细胞数量、白细胞数量等。
一个人身高 180cm，体重 70kg，白细胞计数 ${7.50 \times 10^9 /L}$ ，etc.
衡量两个人的状况时，白细胞计数就会起到主导作用从而遮盖住其他的特征，归一化后就不会有这样的问题。

## 0x02 标准化 Normalization
在这里我们需要强调一下英文翻译的问题，在 Udacity 字幕组中对此进行了探讨：
 `归一化和标准化的英文翻译是一致的，但是根据其用途（或公式）的不同去理解（或翻译）`
 下面我们将探讨最常见的标准化方法：*Z-Score 标准化。*
## Z-score 标准化
$x_{new}=\frac{x-\mu}{\sigma}$
其中$\mu$是样本数据的均值（mean),$\sigma$是样本数据的标准差（std）。
![avatar](https://pic3.zhimg.com/v2-ee0280ea470db277509e95efce1991f6_r.jpg)
上图则是一个散点序列的标准化过程：原图->减去均值->除以标准差。
显而易见，变成了一个==均值为0，方差为1==的分布，下图通过Cost函数让我们更好的理解标准化的作用。
![avarar](https://pic3.zhimg.com/80/v2-7f49cde4e78c482421e17721d2e0fc5e_720w.jpg)
机器学习的目标无非就是不断优化损失函数，使其值最小。在上图中,$J(\omega,b)$就是我们要优化的目标函数
我们不难看出，标准化后可以更加容易地得出最优参数.$\omega$和$b$以及计算出$J(\omega,b)$的最小值，从而达到加速收敛的效果

## 0x03 正则化 Regularization
**正则化主要用于避免过拟合的产生和减少网络误差。**
正则化一般具有如下形式：
$$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m L(f(x),y) + \lambda R(f)$$
其中，第 1 项是**经验风险**，第 2 项是**正则项**， $\lambda >= 0$ 为调整两者之间关系的系数。

第 1 项的经验风险较小的模型可能较复杂（有多个非零参数），这时第 2 项的模型复杂度会较大。

常见的有正则项有 **L1 正则** 和 **L2 正则** ，其中 **L2 正则** 的控制过拟合的效果比 **L1 正则**的好。

**正则化的作用是选择经验风险与模型复杂度同时较小的模型。**

常见的有正则项有 **L1 正则** 和 **L2 正则** 以及 **Dropout** ，其中 **L2 正则** 的控制过拟合的效果比 **L1 正则** 的好。

**$L_p$ 范数**
为什么叫 L1 正则，有 L1、L2 正则 那么有没有 L3、L4 之类的呢？

首先我们补一补课， [公式] 正则的 L 是指 [公式] 范数，其定义为：

$L_0$ 范数：
$$ ||\omega||_0 = *#* with x_i != 0 $$ （非零元素的个数）

$L_1$ 范数： 
$$ ||\omega||_1 = \sum_{i=1}^d |x_i| $$ （每个元素绝对值之和）

$L_2$ 范数： 
$$ ||\omega||_2 = (\sum_{i=1}^d x_i^2)^{\frac{1}{2}} $$ （欧氏距离）

$L_p$ 范数： 
$$ ||\omega||_p = (\sum_{i=1}^d x_i^p)^{\frac{1}{p}} $$

在机器学习中，若使用了 [公式] 作为正则项，我们则说该机器学习任务引入了 $L_p$ 正则项
